
Ollama, Openwebui, Llama.cpp, vLLM  , LM Studio OPenChat

- https://chatgpt.com/c/698d8ae7-3220-8324-b3e1-23e102d5c3bc
- https://huggingface.co/zai-org/GLM-5

### Tutorials

- Local Ai Server Setup Guides Proxmox 9 - OpenWEBUI Ollama in LXC w/ GPU Passthrough - https://www.youtube.com/watch?v=Met9pEfxsF8&list=PLarJAzZsWRGA1Ej0Kxt1z6LAtgtCUWMtO
- Host a Private AI Server at Home with Proxmox Ollama and OpenWebUI https://www.youtube.com/watch?v=y5-6qww8uKk
- Your local LLM is 10x slower than it should be - https://www.youtube.com/watch?v=L9QZ97y9Exg
- OpenClaw with Local LLM - https://www.youtube.com/watch?v=Otn-NbpQH1k
- I tested 17 Uncensored Local LLMs - https://www.youtube.com/watch?v=Yxb-9OWvjkI
- Local AI Agent with LangGraph + Ollama (Full Tutorial, Qwen3) - https://www.youtube.com/watch?v=LXSfjOCYD40


